这篇文章的核心思想在于通过层次化的方法来优化一个互相冲突的目标，本文中这个目标即 性能-公平。
本文事先假定每个智能体的 reward 仅受其占有的系统资源（例如 CPU、Memory 等）的影响。
对于每个智能体来说，其性能越好（自身 reward 越高），则会打破整体的公平性，因为会占用更多的资源。

对于这两个互相冲突的目标，本文采用了层次强化学习的结构。上层的控制器 reward 就是本文设计的包含 fairness 的回报函数。
其动作空间则是一个离散变量 z，表明选择哪一个下层策略。
下层的设计比较有意思。本文将互相冲突的优化目标分解，第一个 sub-policy 负责提升每个智能体的性能，即直接优化环境 reward。
剩下的 n 个 sub-policies 则通过无监督的方式学习，其回报函数是一个人工设计的，具体优化两个指标：
1）sub-policies 之间应该尽可能不同，因而 z 与 o 的互信息应该尽可能大；
2）sub-policies 应该尽可能随机，因而可以尽可能的探索策略空间，从而找到 fairness 的策略。
上述无监督的方法与 UCB 的无监督学习 skills 那篇文章很像，应该就是受其启发。

另外，由于是多智能体系统，通过文章的实验看来每个智能体都是同质的，因而下层 sub-policies 的参数应该是共享的。
**将强化学习中互相冲突的目标通过层次强化的方式去解，这个思路值得借鉴。**
