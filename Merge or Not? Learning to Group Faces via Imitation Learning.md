这篇论文采用逆强化学习算法去进行人脸数据的聚类，个人感觉是一篇信息量很大的论文，整个框架集成了非常多的东西。

由于要用到逆强化学习，因此训练数据是必须的，所以整个算法要划分为训练阶段以及测试阶段，下面先通过测试阶段来熟悉一下
整个算法的流程。整体框架如下图所示：

![](http://o7ie0tcjk.bkt.clouddn.com/zm7whi2lr3vxj4mq.jpg)

首先通过一些经典的聚类算法，本文中比较了K-means、Graph Degree Linkage、Hierarchical Clustering以及
Affinity Propagation等四种算法，选出两个最应该被合并的两个候选类，然后从人脸相似度、类间一致性以及图片质量（侧脸以及检测错的人脸
质量低，正脸质量高）等三个方面来对这两个候选类进行特征提取，这个构成了我们的state，然后通过学习到的策略（根据从IRL学到的reward
进行正常的强化学习流程得到），来判断对这两个候选类是否需要聚类，因此action空间包括{聚类，不聚类}两种动作。动作执行完毕之后，在
回到第一步，这样不断循环直到经典的聚类算法找不到满足某种阈值要求（根据经典聚类算法不同而不同）的两个候选类后，算法终止。

以上是测试阶段。对于训练阶段来说，由于逆强化学习（学徒学习）是需要已知专家经验的，如何产生这种专家经验是问题的核心。本文建模的方法是，
通过已经有的聚类ground-truth，在给定两个候选类的情况下（最开始每张图片都是独立成类），我们显然直到这两个候选类是否应当合并，因此，
专家经验很自然地就得到了。逆强化学习除了专家经验之外，还有很重要的一点是reward function的形式，不同的IRL算法能解决的reward function
种类不同，本文所采取的是最早的IRL算法（Ng和Abbeel于2008年提出），这种方法的局限性在于其reward function必须是线性的，因此本文设计的
就是一个线性的reward function。同样由于算法的局限性，转移矩阵必须已知，这对于本文设计的state以及action空间同样满足，因此action就两种，
合并或者不合并，经过这两种操作之后的状态也是自然而然就知道的。

本文之所以采用IRL，就是觉得人脸聚类这个问题想用强化学习来解决，reward的设计非常重要。同时作者认为这样复杂问题的reward function肯定是
多元的，意思就是说这个函数由多个分量构成，每个分量各有一个权重来控制其对总reward的贡献，而不是简单的正负1。因此如果把reward function设计成多元的，
学习起来也要容易些，多元其实就是多维，因此怎么设计这个特征空间就是接下来要做的，最后作者在reward function的设计上花了很大的功夫。
其具体reward形式设计如下：

![](http://o7ie0tcjk.bkt.clouddn.com/jevjxffj8etl4o6n.jpg)

其中短期回报指的是进行两个候选类合并或者不合并之后的即时回报，这个即时回报就是多元的，需要考虑我们之前提到的三个方面（人脸相似度，类间一致性
及图片质量， 每一方面都是一个多维向量，最后拼接长一个更长的特征向量）。第二个延时回报指的是从当前的聚类状态到ground-truth的聚类状态需要花费
的代价，这里有一个问题在于测试时我们是没有gt的，因此测试时作者表示需要从reward里去除这一项，即beta=0。下面分别详细解释这两部分。

首先是即时回报：

![](http://o7ie0tcjk.bkt.clouddn.com/mvjqgiroixjzykvp.jpg)

如果a是合并那么y(a)=1否则y(a)=-1。如果最后学到的reward要去最大化，那么当a是不合并时后面的线性项需要为负，这样设计可以推动IRL去学习两者的差别。
其实学习这样一个reward其实就是解决一个SVM问题。剩下的就是这个特征项phi(s)要怎么定义了，之前说了有三部分，下面分别讲。

首先是这个人脸相似度，其实是衡量两个候选类C_i, C_j之间的人脸相似度，具体计算方法如下：
1. 对于C_i中的某一张图片x, 计算其与C_j中所有图片的角距离
2. 计算中值
3. 对C_i中其他图片也进行相同操作
4. 选取最小的eta个中值
5. 反过来，以C_j为基准，同样获得eta个中值
6. 将两者合并，得到一个长度为2×eta的向量

第二部分，类间一致性，计算方法如下：
1. 对于C_i中的某一张图片，计算其与C_i中其他图片的角距离
2. 对其他图片进行同样操作
3. 计算所有距离的中值
4. 对C_j中的图片进行同样操作
5. 合并，得到一个长度为2的向量

第三部分，图片质量，计算方法如下:

1. 训练一个线性分类器，类别两类，侧脸以及检测错的人脸为负类，正脸为正类。
2. 对C_i中每张图片进行分类，输出概率即为图片质量
3. 取前eta个值
4. 对C_j进行同样操作
5. 合并，得到一个长度为2×eta的向量

最后得到一个4×eta+2的向量，文中eta=5， 也就是一个22维的特征向量。

然后来讲讲这个延时回报，这个回报指的是从当前聚类状态到gt聚类状态所需要花费的代价，作者把这种聚类状态的转变需要进行的操作分为三种：
1. 向一个类中添加一张图片
2. 从一个类中删除一张图片
3. 合并两个类

作者为了使得这三种操作对应的权重准确，特地找了30个”志愿者“来进行人工分类，最后发现删除操作是其他两种操作的六倍耗时，因此最后的权重为
1:6:1，这个操作也是很厉害了。最后的延时回报是这样定义的：

![](http://o7ie0tcjk.bkt.clouddn.com/bwtc91gkcygejvv9.jpg)

只看K步，应该是为了防止这一项太大从而使得即时回报被忽略掉。

讲了这么多终于要讲到IRL了，其实作者用的这个IRL算法非常简单：

![](http://o7ie0tcjk.bkt.clouddn.com/aqwbdg8nrvzbxews.jpg)

现在的深度逆强化学习用的一般是最大熵IRL，这个算法的推导需要用到概率图模型，但本文用的比较简单（虽然背后的理论直观上难以理解，思想是找到参数使得
feature的expectation最大化，没错是feature的期望，现在我也不是很能理解...）。看到以上算法，其实就是一个解一个SVM问题。注意了，这是算法的
第一阶段，也就是学到reward function，第二阶段要用这个reward function去学习Q-function，然后policy自然就出来了（argmax Q）。第一阶段训练
的时候，作者为了简化训练难度，将延时回报去掉了，同时令gamma=0，这样Q(phi(s), a)=即时回报。这个Q要存下来以备后用。

第二阶段，作者采用一个随机森林回归来拟合Q，这个随机森林采用第一阶段保存的(phi(s), a, Q(phi(s), a))来进行初始化（预训练一下）,预训练之后，将
gamma设为0.9， beta设为0.8（加入延时回报），同时采用epsilon-greedy策略继续进行训练，论文这里说直接用算法采样得到的数据继续拟合随机森林，我觉得
应该是采用Q-learning，但是论文没说。最后训练完毕策略就学到了。



