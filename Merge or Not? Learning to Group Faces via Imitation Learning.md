这篇论文采用逆强化学习算法去进行人脸数据的聚类，个人感觉是一篇信息量很大的论文，整个框架集成了非常多的东西。

由于要用到逆强化学习，因此训练数据是必须的，所以整个算法要划分为训练阶段以及测试阶段，下面先通过测试阶段来熟悉一下
整个算法的流程。整体框架如下图所示：

![](http://o7ie0tcjk.bkt.clouddn.com/zm7whi2lr3vxj4mq.jpg)

首先通过一些经典的聚类算法，本文中比较了K-means、Graph Degree Linkage、Hierarchical Clustering以及
Affinity Propagation等四种算法，选出两个最应该被合并的两个候选类，然后从人脸相似度、类间一致性以及图片质量（侧脸以及检测错的人脸
质量低，正脸质量高）等三个方面来对这两个候选类进行特征提取，这个构成了我们的state，然后通过学习到的策略（根据从IRL学到的reward
进行正常的强化学习流程得到），来判断对这两个候选类是否需要聚类，因此action空间包括{聚类，不聚类}两种动作。动作执行完毕之后，在
回到第一步，这样不断循环直到经典的聚类算法找不到满足某种阈值要求（根据经典聚类算法不同而不同）的两个候选类后，算法终止。

以上是测试阶段。对于训练阶段来说，由于逆强化学习（学徒学习）是需要已知专家经验的，如何产生这种专家经验是问题的核心。本文建模的方法是，
通过已经有的聚类ground-truth，在给定两个候选类的情况下（最开始每张图片都是独立成类），我们显然直到这两个候选类是否应当合并，因此，
专家经验很自然地就得到了。逆强化学习除了专家经验之外，还有很重要的一点是reward function的形式，不同的IRL算法能解决的reward function
种类不同，本文所采取的是最早的IRL算法（Ng和Abbeel于2008年提出），这种方法的局限性在于其reward function必须是线性的，因此本文设计的
就是一个线性的reward function。同样由于算法的局限性，转移矩阵必须已知，这对于本文设计的state以及action空间同样满足，因此action就两种，
合并或者不合并，经过这两种操作之后的状态也是自然而然就知道的。

本文之所以采用IRL，就是觉得人脸聚类这个问题想用强化学习来解决，reward的设计非常重要。同时作者认为这样复杂问题的reward function肯定是
多元的，意思就是说这个函数由多个分量构成，每个分量各有一个权重来控制其对总reward的贡献，而不是简单的正负1。因此如果把reward function设计成多元的，
学习起来也要容易些，多元其实就是多维，因此怎么设计这个特征空间就是接下来要做的，最后作者在reward function的设计上花了很大的功夫。
其具体reward形式设计如下：

![](http://o7ie0tcjk.bkt.clouddn.com/jevjxffj8etl4o6n.jpg)

其中短期回报指的是进行两个候选类合并或者不合并之后的即时回报，这个即时回报就是多元的，需要考虑我们之前提到的三个方面（人脸相似度，类间一致性
及图片质量， 每一方面都是一个多维向量，最后拼接长一个更长的特征向量）。第二个延时回报指的是从当前的聚类状态到ground-truth的聚类状态需要花费
的代价，这里有一个问题在于测试时我们是没有gt的，因此测试时作者表示需要从reward里去除这一项，即beta=0。下面分别详细解释这两部分。

首先是即时回报：

![](http://o7ie0tcjk.bkt.clouddn.com/mvjqgiroixjzykvp.jpg)

如果a是合并那么y(a)=1否则y(a)=-1。如果最后学到的reward要去最大化，那么当a是不合并时后面的线性项需要为负，这样设计可以推动IRL去学习两者的差别。
其实学习这样一个reward其实就是解决一个SVM问题。剩下的就是这个特征项phi(s)要怎么定义了，之前说了有三部分，下面分别讲。

首先是这个人脸相似度，其实是衡量两个候选类C_i, C_j之间的人脸相似度，具体计算方法如下：
1. 对于C_i中的某一张图片x, 计算其与C_j中所有图片的角距离
2. 计算中值
3. 对C_i中其他图片也进行相同操作
4. 选取最小的eta个中值
5. 反过来，以C_j为基准，同样获得eta个中值
6. 将两者合并，得到一个长度为2×eta的向量

第二部分，类间一致性，计算方法如下：
1. 对于C_i中的某一张图片，计算其与C_i中其他图片的角距离
2. 对其他图片进行同样操作
3. 计算所有距离的中值
4. 对C_j中的图片进行同样操作
5. 合并，得到一个长度为2的向量

第三部分，图片质量，计算方法如下:



