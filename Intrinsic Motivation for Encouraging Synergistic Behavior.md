这篇文章解决的是多智能体稀疏回报的环境中如何鼓励智能体探索出协作策略。
这里的协作特指某些场景下多个智能体必须同时进行动作。例如两只手拧开瓶盖，必须一只手握住瓶子的同时，另一只手拧瓶盖。
如果上述联合动作是按顺序执行的，那么首先一只手握紧瓶子松开后，另一只手拧瓶盖。在这个情况下，瓶子会随着瓶盖一起旋转，从而瓶盖无法被拧开。

为了解决上述问题，本文提出了一种 `intrinsic reward`，它是两部分的欧氏距离：
1）第一部分是智能体执行联合动作后在一个 forward model 下转移到的下一个状态；
2）第二部分是智能体按顺序执行两个动作，分别对 forward model 进行两次 inference 后转移到的下一个状态；

这里有几点需要注意：
1）两部分的 forward model 不一样并且是通过监督学习的方式训练得到的
2）引入这种 intrinsic reward 后采用类似 policy gradient 方法时其梯度的计算会有些不同，这里也是难点之一
